#!/bin/bash
# expects that datasource is defined
if [ -z ${ddate+x} ]; then 
  echo "Variable 'ddate' (download date) must be defined when calling raster.config. Exiting."
  exit
fi

db_host="dbase2"
if [ -z "$DB_HOST" ]; then
  dh_host=$DB_HOST # allow override with environment variable
fi
db_port="5432"
db_name="drupal.dh03"
tz=""
MET_SCRIPT_PATH="/opt/model/model_meteorology"
maskExtent='/backup/meteorology/cbp_extent.csv'
yr=`date -d "$ddate" +%Y`
mo=`date -d "$ddate" +%m`
da=`date -d "$ddate" +%d`
jday=`date -d "$ddate" +%j`
ymd="$yr$mo$da"

src_files=""
import_files=""

#Send maskExtent to download functions
export maskExtent

case $datasource in
  PRISM)
  # BEGIN PRISM config
  declare -A config=(
   ["entity_type"]="dh_feature"
   ["ext"]="_CBP.gtiff"
   ["dt"]=86400
   ["scratchdir"]="/tmp"
   ["basedir"]="/backup/meteorology/PRISM"
   ["datasource"]="PRISM"
   ["dataset"]="PRISM_precip_"
   ["varkey"]="prism_mod_daily"
   ["extent_hydrocode"]="cbp6_met_coverage"
   ["extent_ftype"]="cbp_met_grid"
   ["extent_bundle"]="landunit"
  )
  final_ext=${config["ext"]}
  src_dir="$base_dir/$yr/$jday"
  if [ -e $src_dir ]; then
    import_files=`ls $src_dir/*.gtiff | grep -v repro.gtiff`

    src_files=`ls $src_dir/*.bil`
  fi
  # END PRISM config
  ;;

  daymet)
  # BEGIN daymet config
  # dayment config array goes here.
  # this code block must set variables named:
  # - import_files: a space delimited list of file paths to the geotiff files to import for each day
  #                 daymet should have one file for each day as it's a daily dataset
  # - src_files: a space delimited list of file paths to send to the convert to tiff step
  #              this set of files is maybe superfluous, as the download *coudl* convert to tiff
  # 
  # END daymet config
  ;;
  
  daymet)
  declare -A config=(
   ["basedir"]="/backup/meteorology/daymet"
   ["entity_type"]="dh_feature"
   ["ext"]=".gtiff"
   ["scratchdir"]="/tmp"
   ["dataset"]="daymet_precip_"
   ["datasource"]="daymet"
   ["varkey"]="daymet_mod_daily"
   ["extent_hydrocode"]="cbp6_met_coverage"
   ["extent_ftype"]="cbp_met_grid"
   ["extent_bundle"]="landunit"
   ["timezone"]="UTC"
   ["dt"]=86400
  )
  # files to import can be inferred
  final_ext=${config["ext"]}
  met_dateset=${config["dataset"]}
  if [ -d "$base_dir/$yr" ]; then
    if [ -d "$base_dir/$yr/$jday" ]; then
      src_dir="$base_dir/$yr/$jday"
      src_files=`ls $src_dir/*.gtiff`
      # this will have a list of geotiff if the files have been processed
      # but will omit intermediates and the gtiffs created from the netCDF
      # extraction
      import_files=`ls $src_dir/*.gtiff | grep -v repro.gtiff | grep -v ORIGINAL_ ` 
      
      # specify the location to store things in
      archive_dir="$base_dir/$yr"
    fi
  fi
  ;;
  

  nldas2)
  declare -A config=(
   ["entity_type"]="dh_feature"
   ["ext"]="_CBP.gtiff"
   ["dt"]=3600
   ["TZ"]="UTC"
   ["scratchdir"]="/tmp"
   ["basedir"]="/backup/meteorology"
   ["datasource"]="nldas2"
   ["dataset"]="nldas2_precip_"
   ["varkey"]="nldas2_obs_hourly"
   ["extent_hydrocode"]="cbp6_met_coverage"
   ["extent_ftype"]="cbp_met_grid"
   ["extent_bundle"]="landunit"
  )
  # files to import can be inferred
  final_ext=${config["ext"]}
  echo "Looking for raster files in $base_dir/$yr/$jday"
  if [ -d "$base_dir/$yr" ]; then
    if [ -d "$base_dir/$yr/$jday" ]; then
      src_dir="$base_dir/$yr/$jday"
      src_files=`ls $src_dir/*.grb`
      # this will have a list of geotiff if the files have been processed
      # but will omit intermediates.
      import_files=`ls $src_dir/*.gtiff | grep -v repro.gtiff `
    fi
  fi
  ;;
esac

# create friendly names
datasource=${config["datasource"]}
extent_hydrocode=${config["extent_hydrocode"]}
extent_bundle=${config["extent_bundle"]}
varkey=${config["varkey"]}
extent_ftype=${config["extent_ftype"]}
base_dir=${config["basedir"]}
entity_type=${config["entity_type"]}
